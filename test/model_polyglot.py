# model.py
from transformers import AutoTokenizer, AutoModelForCausalLM

MODEL_NAME = "EleutherAI/polyglot-ko-1.3B"

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)

def generate_answer(user_input: str) -> str:
    system_prompt = (
        "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ì •ì¤‘í•œ ë§íˆ¬ë¡œ ë‹µë³€í•˜ëŠ” í•œêµ­ì–´ ì¸ê³µì§€ëŠ¥ ë¹„ì„œì…ë‹ˆë‹¤.\n"
        "ëª¨ë“  ë¬¸ì¥ì€ ê³µì‹ì ì¸ ì–´íˆ¬(ì…ë‹ˆë‹¤/ìŠµë‹ˆë‹¤)ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.\n"
        "ë§ì¶¤ë²•ê³¼ ë¬¸ë²•ì„ ì˜¬ë°”ë¥´ê²Œ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.\n"
        "ì§ˆë¬¸ì— ëŒ€í•´ ê°„ê²°í•˜ë©´ì„œë„ í•µì‹¬ ì •ë³´ë¥¼ ë¹ ì§ì—†ì´ ì „ë‹¬í•˜ì‹­ì‹œì˜¤.\n\n"
        "5ë¬¸ì¥ ì´ë‚´ë¡œ ë‹µë³€í•˜ë©°, í•„ìš”í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì œì™¸í•˜ì‹­ì‹œì˜¤.\n\n"
        "ë‹µë³€ì´ ëë‚˜ë©´, ì˜¨ì  ë’¤ì˜ íŠ¹ìˆ˜ë¬¸ìëŠ” ì œì™¸í•˜ì‹­ì‹œì˜¤.\n\n"
        f"ì§ˆë¬¸: {user_input}\në‹µë³€:"
    )

    inputs = tokenizer(system_prompt, return_tensors="pt")
    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]

    outputs = model.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,
        max_new_tokens=128,
        do_sample=False,
        temperature=0.8,
        top_k=40
    )
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return result.replace(system_prompt, "").strip()


# âœ… í…ŒìŠ¤íŠ¸ CLI ì‹¤í–‰
# if __name__ == "__main__":
#     print("ğŸ¤– ëª¨ë¸ ë¡œë”© ì™„ë£Œ (Polyglot-ko)")
#     while True:
#         prompt = input("ğŸ’¬ ì§ˆë¬¸ ì…ë ¥ (ì¢…ë£Œ: exit): ")
#         if prompt.lower() == "exit":
#             break
#         response = generate_answer(prompt)
#         print("ğŸ¤– ë‹µë³€:", response)
